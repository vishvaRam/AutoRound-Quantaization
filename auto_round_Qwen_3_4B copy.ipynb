{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-round --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8040c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from auto_round import AutoRound\n",
    "from huggingface_hub import HfApi, create_repo, notebook_login, get_token\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cddaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6517109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.10.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU Name: NVIDIA GeForce RTX 3090 Ti\n",
      "VRAM: 23.5 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b05d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "HF_USER = \"Vishva007\"\n",
    "OUTPUT_BASE_DIR = \"./AutoRound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4920035b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c8456b5e8e43aa9d8257425e6e09fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1063ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"AWQ\",            # Best for Nvidia GPUs (vLLM, TGI)\n",
    "# \"GPTQ\",           # Good compatibility\n",
    "# \"AutoRound\",      # Intel default format (Requires auto-round lib to run)\n",
    "# \"GGUF\",           # For llama.cpp / Olama\n",
    "\n",
    "TARGET_FORMATS = \"AWQ\"\n",
    "\n",
    "# High-End GPU Tuning Parameters (A40/A6000/L40)\n",
    "TUNING_CONFIG = {\n",
    "    \"group_size\": 128,\n",
    "    \"sym\": True,\n",
    "    \"iters\": 1000,          # High accuracy (Production grade)\n",
    "    \"nsamples\": 512,        # More calibration data\n",
    "    \"batch_size\": 4,        # Faster on 48GB VRAM\n",
    "    \"seqlen\": 2048,\n",
    "    \"low_gpu_mem_usage\": False,   # Keep on GPU for speed\n",
    "    \"enable_torch_compile\": True, # JIT acceleration\n",
    "    # \"quant_nontext_module\": False # Keep Vision Tower in BF16 (Crucial for VLM accuracy)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d4d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(local_dir, repo_name, token):\n",
    "    \"\"\"Creates repo and uploads folder to Hugging Face.\"\"\"\n",
    "    full_repo_id = f\"{HF_USER}/{repo_name}\"\n",
    "    print(f\"\\n[Hub] Pushing {local_dir} to {full_repo_id}...\")\n",
    "    \n",
    "    try:\n",
    "        api = HfApi()\n",
    "        create_repo(full_repo_id, repo_type=\"model\", exist_ok=True, private=False, token=token)\n",
    "        \n",
    "        api.upload_folder(\n",
    "            folder_path=local_dir,\n",
    "            repo_id=full_repo_id,\n",
    "            repo_type=\"model\",\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"[Hub] ✅ Successfully uploaded: https://huggingface.co/{full_repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Hub] ❌ Error uploading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df73279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e9f9bfc26041be8d453a60cec6c3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_fp16 = TARGET_FORMATS in [\"AWQ\", \"GPTQ\"]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    dtype=torch.float16 if use_fp16 else \"auto\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d0a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2026-02-01 06:29:55 INFO base.py L391: using torch.float16 for quantization tuning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ar = AutoRound(\n",
    "        model,              \n",
    "        tokenizer, \n",
    "        scheme=\"W4A16\",\n",
    "        **TUNING_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec12fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2026-02-01 06:29:56 INFO base.py L1729: start to cache block inputs\u001b[0m\n",
      "\u001b[38;20m2026-02-01 06:30:05 INFO base.py L1744: caching done\u001b[0m\n",
      "Quantizing model.layers.0:   0%|          | 0/36 [00:01<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:865: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:114.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000152 -> iter 755: 0.000043,'peak_ram': 13.64GB, 'peak_vram': 19.24GB\u001b[0m\n",
      "Quantizing model.layers.1:   3%|▎         | 1/36 [01:37<56:35, 97.03s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000393 -> iter 778: 0.000098,'peak_ram': 13.64GB, 'peak_vram': 20.4GB\u001b[0m\n",
      "Quantizing model.layers.2:   6%|▌         | 2/36 [03:02<51:16, 90.49s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000617 -> iter 509: 0.000150,'peak_ram': 13.64GB, 'peak_vram': 20.4GB\u001b[0m\n",
      "Quantizing model.layers.3:   8%|▊         | 3/36 [04:29<48:51, 88.83s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000949 -> iter 990: 0.000254,'peak_ram': 13.64GB, 'peak_vram': 20.4GB\u001b[0m\n",
      "Quantizing model.layers.4:  11%|█         | 4/36 [05:56<46:57, 88.05s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.001584 -> iter 678: 0.000506,'peak_ram': 13.64GB, 'peak_vram': 20.4GB\u001b[0m\n",
      "Quantizing model.layers.5:  14%|█▍        | 5/36 [07:24<45:22, 87.81s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.002110 -> iter 809: 0.000837,'peak_ram': 13.64GB, 'peak_vram': 20.4GB\u001b[0m\n",
      "Quantizing model.layers.6:  17%|█▋        | 6/36 [08:51<43:50, 87.69s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.113156 -> iter 853: 0.002442,'peak_ram': 13.64GB, 'peak_vram': 20.4GB\u001b[0m\n",
      "Quantizing model.layers.7:  19%|█▉        | 7/36 [10:19<42:21, 87.65s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.006631 -> iter 678: 0.003215,'peak_ram': 13.64GB, 'peak_vram': 20.43GB\u001b[0m\n",
      "Quantizing model.layers.8:  22%|██▏       | 8/36 [11:46<40:53, 87.61s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.007282 -> iter 771: 0.003542,'peak_ram': 13.64GB, 'peak_vram': 20.55GB\u001b[0m\n",
      "Quantizing model.layers.9:  25%|██▌       | 9/36 [13:14<39:25, 87.60s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.007668 -> iter 906: 0.004173,'peak_ram': 13.64GB, 'peak_vram': 20.55GB\u001b[0m\n",
      "Quantizing model.layers.10:  28%|██▊       | 10/36 [14:41<37:57, 87.61s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.007192 -> iter 504: 0.004253,'peak_ram': 13.64GB, 'peak_vram': 20.55GB\u001b[0m\n",
      "Quantizing model.layers.11:  31%|███       | 11/36 [16:09<36:32, 87.72s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.007248 -> iter 368: 0.004278,'peak_ram': 13.64GB, 'peak_vram': 20.55GB\u001b[0m\n",
      "Quantizing model.layers.12:  33%|███▎      | 12/36 [17:37<35:04, 87.67s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.010345 -> iter 288: 0.005259,'peak_ram': 13.64GB, 'peak_vram': 20.61GB\u001b[0m\n",
      "Quantizing model.layers.13:  36%|███▌      | 13/36 [19:04<33:35, 87.61s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.008571 -> iter 898: 0.005913,'peak_ram': 13.64GB, 'peak_vram': 20.61GB\u001b[0m\n",
      "Quantizing model.layers.14:  39%|███▉      | 14/36 [20:32<32:06, 87.56s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.012571 -> iter 812: 0.006654,'peak_ram': 13.64GB, 'peak_vram': 20.66GB\u001b[0m\n",
      "Quantizing model.layers.15:  42%|████▏     | 15/36 [21:59<30:39, 87.61s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.013045 -> iter 988: 0.007388,'peak_ram': 13.64GB, 'peak_vram': 20.66GB\u001b[0m\n",
      "Quantizing model.layers.16:  44%|████▍     | 16/36 [23:27<29:12, 87.63s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.031878 -> iter 994: 0.017498,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.17:  47%|████▋     | 17/36 [24:55<27:46, 87.71s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.280355 -> iter 823: 0.016503,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.18:  50%|█████     | 18/36 [26:22<26:16, 87.61s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.024660 -> iter 742: 0.018861,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.19:  53%|█████▎    | 19/36 [27:50<24:48, 87.56s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.631236 -> iter 311: 0.023246,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.20:  56%|█████▌    | 20/36 [29:17<23:21, 87.58s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.044414 -> iter 404: 0.026769,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.21:  58%|█████▊    | 21/36 [30:45<21:54, 87.62s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.943303 -> iter 865: 0.032638,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.22:  61%|██████    | 22/36 [32:13<20:26, 87.64s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.986498 -> iter 915: 0.036062,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.23:  64%|██████▍   | 23/36 [33:41<18:59, 87.67s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 3.236399 -> iter 869: 0.050978,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.24:  67%|██████▋   | 24/36 [35:08<17:32, 87.70s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 4.887598 -> iter 967: 0.065977,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.25:  69%|██████▉   | 25/36 [36:36<16:04, 87.73s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 4.307837 -> iter 842: 0.080224,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.26:  72%|███████▏  | 26/36 [38:04<14:37, 87.74s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.384120 -> iter 377: 0.097348,'peak_ram': 13.64GB, 'peak_vram': 20.71GB\u001b[0m\n",
      "Quantizing model.layers.27:  75%|███████▌  | 27/36 [39:32<13:09, 87.75s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.742647 -> iter 752: 0.117425,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.28:  78%|███████▊  | 28/36 [40:59<11:41, 87.73s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.290087 -> iter 878: 0.142118,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.29:  81%|████████  | 29/36 [42:28<10:16, 88.00s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.775317 -> iter 989: 0.212886,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.30:  83%|████████▎ | 30/36 [43:57<08:49, 88.18s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.618775 -> iter 675: 0.277305,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.31:  86%|████████▌ | 31/36 [45:25<07:21, 88.35s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 3.035076 -> iter 666: 0.331114,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.32:  89%|████████▉ | 32/36 [46:54<05:53, 88.41s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.951629 -> iter 335: 0.442831,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.33:  92%|█████████▏| 33/36 [48:22<04:25, 88.40s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.462177 -> iter 868: 0.530412,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.34:  94%|█████████▍| 34/36 [49:51<02:56, 88.41s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.970755 -> iter 463: 0.731595,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing model.layers.35:  97%|█████████▋| 35/36 [51:19<01:28, 88.43s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 4.106770 -> iter 218: 1.527096,'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "Quantizing done: 100%|██████████| 36/36 [52:48<00:00, 88.02s/it]           \n",
      "\u001b[38;20m2026-02-01 07:22:54 INFO device.py L1430: 'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n",
      "\u001b[38;20m2026-02-01 07:22:54 INFO base.py L1794: quantization tuning time 3177.301817178726\u001b[0m\n",
      "\u001b[38;20m2026-02-01 07:22:54 INFO base.py L1812: Summary: quantized 252/253 in the model,  ['lm_head'] have not been quantized\u001b[0m\n",
      "\u001b[38;20m2026-02-01 07:22:54 INFO export.py L96: Saving quantized model to auto_awq format\u001b[0m\n",
      "packing model.layers.35.mlp.down_proj: 100%|██████████| 252/252 [00:03<00:00, 75.44it/s]   \n",
      "\u001b[38;20m2026-02-01 07:23:01 INFO device.py L1430: 'peak_ram': 13.64GB, 'peak_vram': 20.72GB\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Qwen3ForCausalLM(\n",
       "   (model): Qwen3Model(\n",
       "     (embed_tokens): Embedding(151936, 2560)\n",
       "     (layers): ModuleList(\n",
       "       (0-35): 36 x Qwen3DecoderLayer(\n",
       "         (self_attn): Qwen3Attention(\n",
       "           (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "           (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "           (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "           (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "           (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "           (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "         )\n",
       "         (mlp): Qwen3MLP(\n",
       "           (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "           (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "           (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "           (act_fn): SiLUActivation()\n",
       "         )\n",
       "         (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "         (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "     (rotary_emb): Qwen3RotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       " ),\n",
       " ['./AutoRound/W4A16-AWQ'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_name = MODEL_ID.split(\"/\")[-1]\n",
    "\n",
    "if TARGET_FORMATS == \"AWQ\":\n",
    "    dir_suffix = \"W4A16-AWQ\"\n",
    "    format_arg = \"auto_awq\"\n",
    "elif TARGET_FORMATS == \"GPTQ\":\n",
    "    dir_suffix = \"W4A16-GPTQ\"\n",
    "    format_arg = \"auto_gptq\"\n",
    "else: # AutoRound\n",
    "    dir_suffix = \"W4A16-AutoRound\"\n",
    "    format_arg = \"auto_round\"\n",
    "\n",
    "save_dir = os.path.join(OUTPUT_BASE_DIR, dir_suffix)\n",
    "\n",
    "\n",
    "ar.quantize_and_save(\n",
    "    format=format_arg, \n",
    "    output_dir=save_dir, \n",
    "    inplace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6022046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hub] Pushing ./AutoRound/W4A16-AWQ to Vishva007/Qwen3-4B-Instruct-2507-W4A16-AutoRound-AWQ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03ad498f56949a094eb86723dae6cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c78fb9132824cba80e36b7f9b96abea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hub] ✅ Successfully uploaded: https://huggingface.co/Vishva007/Qwen3-4B-Instruct-2507-W4A16-AutoRound-AWQ\n"
     ]
    }
   ],
   "source": [
    "hf_token = get_token()\n",
    "\n",
    "if hf_token:\n",
    "    push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound-{TARGET_FORMATS}\", hf_token)\n",
    "else:\n",
    "    print(\"❌ No HF token found. Skipping upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aba13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
