{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-round --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8040c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from auto_round import AutoRound\n",
    "from huggingface_hub import HfApi, create_repo, notebook_login, get_token\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cddaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6517109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.10.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU Name: NVIDIA RTX A4000\n",
      "VRAM: 15.7 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b05d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-1.7B\"\n",
    "HF_USER = \"Vishva007\"\n",
    "OUTPUT_BASE_DIR = \"./AutoRound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4920035b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b120930cb4543c58923a2c0f92a47d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1063ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"AWQ\",            # Best for Nvidia GPUs (vLLM, TGI)\n",
    "# \"GPTQ\",           # Good compatibility\n",
    "# \"AutoRound\",      # Intel default format (Requires auto-round lib to run)\n",
    "# \"GGUF\",           # For llama.cpp / Olama\n",
    "\n",
    "TARGET_FORMATS = \"AWQ\"\n",
    "\n",
    "# High-End GPU Tuning Parameters (A40/A6000/L40)\n",
    "TUNING_CONFIG = {\n",
    "    \"group_size\": 128,\n",
    "    \"sym\": True,\n",
    "    \"iters\": 256,          # High accuracy (Production grade)\n",
    "    \"nsamples\": 256,        # More calibration data\n",
    "    \"batch_size\": 8,        # Faster on 48GB VRAM\n",
    "    \"seqlen\": 2048,\n",
    "    \"low_gpu_mem_usage\": False,   # Keep on GPU for speed\n",
    "    \"enable_torch_compile\": True, # JIT acceleration\n",
    "    # \"quant_nontext_module\": False # Keep Vision Tower in BF16 (Crucial for VLM accuracy)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d4d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(local_dir, repo_name, token):\n",
    "    \"\"\"Creates repo and uploads folder to Hugging Face.\"\"\"\n",
    "    full_repo_id = f\"{HF_USER}/{repo_name}\"\n",
    "    print(f\"\\n[Hub] Pushing {local_dir} to {full_repo_id}...\")\n",
    "    \n",
    "    try:\n",
    "        api = HfApi()\n",
    "        create_repo(full_repo_id, repo_type=\"model\", exist_ok=True, private=False, token=token)\n",
    "        \n",
    "        api.upload_folder(\n",
    "            folder_path=local_dir,\n",
    "            repo_id=full_repo_id,\n",
    "            repo_type=\"model\",\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"[Hub] ✅ Successfully uploaded: https://huggingface.co/{full_repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Hub] ❌ Error uploading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df73279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb3f0d45c3b438b8142f18fdb65bb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_fp16 = TARGET_FORMATS in [\"AWQ\", \"GPTQ\"]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    dtype=torch.float16 if use_fp16 else \"auto\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d0a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2026-02-01 05:23:55 INFO base.py L391: using torch.float16 for quantization tuning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ar = AutoRound(\n",
    "        model,              \n",
    "        tokenizer, \n",
    "        scheme=\"W4A16\",\n",
    "        **TUNING_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2026-02-01 05:23:56 INFO base.py L1729: start to cache block inputs\u001b[0m\n",
      "\u001b[38;20m2026-02-01 05:24:02 INFO base.py L1744: caching done\u001b[0m\n",
      "Quantizing model.layers.0:   0%|          | 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:865: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:114.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.002010 -> iter 224: 0.000584,'peak_ram': 6.33GB, 'peak_vram': 7.9GB\u001b[0m\n",
      "Quantizing model.layers.1:   4%|▎         | 1/28 [00:33<14:59, 33.30s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.003459 -> iter 234: 0.001175,'peak_ram': 6.33GB, 'peak_vram': 7.94GB\u001b[0m\n",
      "Quantizing model.layers.2:   7%|▋         | 2/28 [01:04<13:55, 32.14s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.049776 -> iter 90: 0.024743,'peak_ram': 6.33GB, 'peak_vram': 7.94GB\u001b[0m\n",
      "Quantizing model.layers.3:  11%|█         | 3/28 [01:35<13:09, 31.56s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.035735 -> iter 239: 0.024846,'peak_ram': 6.33GB, 'peak_vram': 7.94GB\u001b[0m\n",
      "Quantizing model.layers.4:  14%|█▍        | 4/28 [02:06<12:33, 31.38s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.037942 -> iter 158: 0.027446,'peak_ram': 6.33GB, 'peak_vram': 7.94GB\u001b[0m\n",
      "Quantizing model.layers.5:  18%|█▊        | 5/28 [02:37<11:59, 31.26s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.039700 -> iter 215: 0.030204,'peak_ram': 6.33GB, 'peak_vram': 7.94GB\u001b[0m\n",
      "Quantizing model.layers.6:  21%|██▏       | 6/28 [03:08<11:26, 31.21s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.047571 -> iter 218: 0.034210,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.7:  25%|██▌       | 7/28 [03:39<10:55, 31.20s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.052936 -> iter 206: 0.039050,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.8:  29%|██▊       | 8/28 [04:11<10:23, 31.19s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.069309 -> iter 186: 0.045328,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.9:  32%|███▏      | 9/28 [04:42<09:53, 31.22s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.084473 -> iter 194: 0.058924,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.10:  36%|███▌      | 10/28 [05:13<09:22, 31.24s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.133815 -> iter 175: 0.082734,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.11:  39%|███▉      | 11/28 [05:44<08:51, 31.24s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.198785 -> iter 242: 0.116661,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.12:  43%|████▎     | 12/28 [06:16<08:19, 31.25s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.219799 -> iter 192: 0.148151,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.13:  46%|████▋     | 13/28 [06:47<07:48, 31.24s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.292839 -> iter 177: 0.197077,'peak_ram': 6.33GB, 'peak_vram': 7.97GB\u001b[0m\n",
      "Quantizing model.layers.14:  50%|█████     | 14/28 [07:18<07:17, 31.25s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.403426 -> iter 253: 0.266371,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.15:  54%|█████▎    | 15/28 [07:49<06:46, 31.26s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.567241 -> iter 226: 0.366489,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.16:  57%|█████▋    | 16/28 [08:21<06:15, 31.26s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.259976 -> iter 238: 0.669118,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.17:  61%|██████    | 17/28 [08:52<05:44, 31.29s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.634014 -> iter 183: 1.096782,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.18:  64%|██████▍   | 18/28 [09:23<05:13, 31.31s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.497041 -> iter 145: 1.729676,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.19:  68%|██████▊   | 19/28 [09:55<04:42, 31.36s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 4.463735 -> iter 231: 2.601118,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.20:  71%|███████▏  | 20/28 [10:26<04:11, 31.41s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 6.459427 -> iter 130: 4.422343,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.21:  75%|███████▌  | 21/28 [10:58<03:40, 31.45s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 9.882713 -> iter 255: 6.664568,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.22:  79%|███████▊  | 22/28 [11:29<03:08, 31.44s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 13.598451 -> iter 56: 9.361561,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.23:  82%|████████▏ | 23/28 [12:01<02:37, 31.48s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 18.971828 -> iter 162: 11.673240,'peak_ram': 6.33GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.24:  86%|████████▌ | 24/28 [12:32<02:05, 31.47s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 26.637688 -> iter 195: 17.176201,'peak_ram': 6.4GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.25:  89%|████████▉ | 25/28 [13:04<01:34, 31.50s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 37.806801 -> iter 188: 21.947741,'peak_ram': 6.46GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.26:  93%|█████████▎| 26/28 [13:36<01:03, 31.52s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 48.306076 -> iter 181: 26.292824,'peak_ram': 6.53GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing model.layers.27:  96%|█████████▋| 27/28 [14:07<00:31, 31.52s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 88.961960 -> iter 198: 40.236816,'peak_ram': 6.6GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "Quantizing done: 100%|██████████| 28/28 [14:39<00:00, 31.41s/it]           \n",
      "\u001b[38;20m2026-02-01 05:38:41 INFO device.py L1430: 'peak_ram': 6.6GB, 'peak_vram': 8.04GB\u001b[0m\n",
      "\u001b[38;20m2026-02-01 05:38:41 INFO base.py L1794: quantization tuning time 885.2473618984222\u001b[0m\n",
      "\u001b[38;20m2026-02-01 05:38:41 INFO base.py L1812: Summary: quantized 196/197 in the model,  ['lm_head'] have not been quantized\u001b[0m\n",
      "\u001b[38;20m2026-02-01 05:38:41 INFO export.py L96: Saving quantized model to auto_awq format\u001b[0m\n",
      "packing model.layers.27.mlp.down_proj: 100%|██████████| 196/196 [00:01<00:00, 118.62it/s]   \n",
      "\u001b[38;20m2026-02-01 05:38:47 INFO device.py L1430: 'peak_ram': 6.6GB, 'peak_vram': 8.04GB\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Qwen3ForCausalLM(\n",
       "   (model): Qwen3Model(\n",
       "     (embed_tokens): Embedding(151936, 2048)\n",
       "     (layers): ModuleList(\n",
       "       (0-27): 28 x Qwen3DecoderLayer(\n",
       "         (self_attn): Qwen3Attention(\n",
       "           (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "           (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "           (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "           (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "         )\n",
       "         (mlp): Qwen3MLP(\n",
       "           (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "           (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "           (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "           (act_fn): SiLUActivation()\n",
       "         )\n",
       "         (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "         (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "     (rotary_emb): Qwen3RotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       " ),\n",
       " ['./AutoRound/W4A16-AWQ'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_name = MODEL_ID.split(\"/\")[-1]\n",
    "\n",
    "if TARGET_FORMATS == \"AWQ\":\n",
    "    dir_suffix = \"W4A16-AWQ\"\n",
    "    format_arg = \"auto_awq\"\n",
    "elif TARGET_FORMATS == \"GPTQ\":\n",
    "    dir_suffix = \"W4A16-GPTQ\"\n",
    "    format_arg = \"auto_gptq\"\n",
    "else: # AutoRound\n",
    "    dir_suffix = \"W4A16-AutoRound\"\n",
    "    format_arg = \"auto_round\"\n",
    "\n",
    "save_dir = os.path.join(OUTPUT_BASE_DIR, dir_suffix)\n",
    "\n",
    "\n",
    "ar.quantize_and_save(\n",
    "    format=format_arg, \n",
    "    output_dir=save_dir, \n",
    "    inplace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6022046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hub] Pushing ./AutoRound/W4A16-AWQ to Vishva007/Qwen3-1.7B-W4A16-AutoRound-AWQ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21cc8e1eb564d5d96d65d70110c0112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03f39efc0ed43b5be4138e211c7b410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hub] ✅ Successfully uploaded: https://huggingface.co/Vishva007/Qwen3-1.7B-W4A16-AutoRound-AWQ\n"
     ]
    }
   ],
   "source": [
    "hf_token = get_token()\n",
    "\n",
    "if hf_token:\n",
    "    push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound-{TARGET_FORMATS}\", hf_token)\n",
    "else:\n",
    "    print(\"❌ No HF token found. Skipping upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aba13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
