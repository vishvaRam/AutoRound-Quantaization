{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8040c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from auto_round import AutoRound\n",
    "from huggingface_hub import HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cddaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6517109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU Name: NVIDIA L40\n",
      "VRAM: 44.3 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b05d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
    "HF_USER = \"Vishva007\"\n",
    "OUTPUT_BASE_DIR = \"./AutoRound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4920035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1063ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FORMATS = [\n",
    "    \"AWQ\",            # Best for Nvidia GPUs (vLLM, TGI)\n",
    "    \"GPTQ\",           # Good compatibility\n",
    "    \"AutoRound\",      # Intel default format (Requires auto-round lib to run)\n",
    "    # \"GGUF\",         # For llama.cpp / Olama\n",
    "]\n",
    "\n",
    "# High-End GPU Tuning Parameters (A40/A6000/L40)\n",
    "TUNING_CONFIG = {\n",
    "    \"group_size\": 128,\n",
    "    \"sym\": True,\n",
    "    \"iters\": 1000,          # High accuracy (Production grade)\n",
    "    \"nsamples\": 512,        # More calibration data\n",
    "    \"batch_size\": 8,        # Faster on 48GB VRAM\n",
    "    \"seqlen\": 2048,\n",
    "    \"low_gpu_mem_usage\": False,   # Keep on GPU for speed\n",
    "    \"enable_torch_compile\": True, # JIT acceleration\n",
    "    \"quant_nontext_module\": False # Keep Vision Tower in BF16 (Crucial for VLM accuracy)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d4d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(local_dir, repo_name, token):\n",
    "    \"\"\"Creates repo and uploads folder to Hugging Face.\"\"\"\n",
    "    full_repo_id = f\"{HF_USER}/{repo_name}\"\n",
    "    print(f\"\\n[Hub] Pushing {local_dir} to {full_repo_id}...\")\n",
    "    \n",
    "    try:\n",
    "        api = HfApi(token=token)\n",
    "        create_repo(full_repo_id, repo_type=\"model\", exist_ok=True, private=False, token=token)\n",
    "        \n",
    "        api.upload_folder(\n",
    "            folder_path=local_dir,\n",
    "            repo_id=full_repo_id,\n",
    "            repo_type=\"model\",\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"[Hub] ✅ Successfully uploaded: https://huggingface.co/{full_repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Hub] ❌ Error uploading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d0a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2026-01-30 17:01:45 INFO autoround.py L158: using MLLM mode for multimodal model.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bd21aa498d4e6b85b2f953be93d63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2026-01-30 17:01:53 INFO base.py L391: using torch.bfloat16 for quantization tuning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ar = AutoRound(\n",
    "        MODEL_ID,\n",
    "        scheme=\"W4A16\",\n",
    "        **TUNING_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.0.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.0.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.1.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.1.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.2.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.2.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.3.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.3.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.4.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.4.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.5.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.5.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.6.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.6.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.7.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.7.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.8.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.8.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.9.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.9.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.10.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.10.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.11.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.11.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.12.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.12.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.13.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.13.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.14.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.14.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.15.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.15.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.16.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.16.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.17.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.17.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.18.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.18.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.19.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.19.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.20.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.20.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.21.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.21.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.22.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.22.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.23.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.23.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.24.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.24.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.25.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.25.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.26.mlp.linear_fc1 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING utils.py L425: model.visual.blocks.26.mlp.linear_fc2 skipped quantization (shape not divisible by 32).\u001b[0m\n",
      "\u001b[33;1m2026-01-30 17:02:05 WARNING base.py L1685: this API is deprecated, please use `quantize_and_save` instead\u001b[0m\n",
      "\u001b[38;20m2026-01-30 17:02:05 INFO base.py L1729: start to cache block inputs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a165e215a84771b180d8891ab7ce0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4418c22e2d994fd2a965bed05220c757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8384c1940a445b99be1bb2001371d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-4746b8785c874c(…):   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ba00ef82574739a1a7e4680fa328ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e17ca85dd11415595c827d58b03854f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8938991ffcc4207b5fc993a7c1f39ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4842eab34ff249efb38c8e13cee73a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cache block inputs: 100%|██████████| 512/512 [00:01<00:00, 272.40it/s]\n",
      "\u001b[38;20m2026-01-30 17:03:06 INFO base.py L1744: caching done\u001b[0m\n",
      "Quantizing model.language_model.layers.0:   0%|          | 0/36 [00:02<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:829: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:110.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000117 -> iter 738: 0.000050,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.1:   3%|▎         | 1/36 [04:11<2:26:37, 251.36s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000320 -> iter 728: 0.000091,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.2:   6%|▌         | 2/36 [07:46<2:10:17, 229.91s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000388 -> iter 497: 0.000110,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.3:   8%|▊         | 3/36 [11:22<2:02:59, 223.62s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000801 -> iter 940: 0.000262,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.4:  11%|█         | 4/36 [15:00<1:58:09, 221.54s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.001440 -> iter 784: 0.000486,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.5:  14%|█▍        | 5/36 [18:39<1:53:58, 220.60s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.002376 -> iter 539: 0.000830,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.6:  17%|█▋        | 6/36 [22:18<1:49:57, 219.92s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.040980 -> iter 718: 0.004700,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.7:  19%|█▉        | 7/36 [25:56<1:45:57, 219.22s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.023568 -> iter 625: 0.005164,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.8:  22%|██▏       | 8/36 [29:33<1:42:04, 218.73s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.009031 -> iter 806: 0.005530,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.9:  25%|██▌       | 9/36 [33:10<1:38:12, 218.25s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.010718 -> iter 945: 0.006243,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.10:  28%|██▊       | 10/36 [36:47<1:34:24, 217.88s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.016620 -> iter 877: 0.005896,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.11:  31%|███       | 11/36 [40:27<1:30:56, 218.26s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.011060 -> iter 507: 0.006373,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.12:  33%|███▎      | 12/36 [44:03<1:27:04, 217.69s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.011315 -> iter 815: 0.007325,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.13:  36%|███▌      | 13/36 [47:39<1:23:18, 217.30s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.013863 -> iter 510: 0.008264,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.14:  39%|███▉      | 14/36 [51:16<1:19:34, 217.01s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.013549 -> iter 923: 0.008716,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.15:  42%|████▏     | 15/36 [54:52<1:15:54, 216.87s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.016084 -> iter 937: 0.009950,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.16:  44%|████▍     | 16/36 [58:29<1:12:14, 216.75s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.550858 -> iter 858: 0.023416,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.17:  47%|████▋     | 17/36 [1:02:05<1:08:34, 216.55s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.328956 -> iter 996: 0.023081,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.18:  50%|█████     | 18/36 [1:05:41<1:04:55, 216.43s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 5.376561 -> iter 353: 0.028438,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.19:  53%|█████▎    | 19/36 [1:09:17<1:01:18, 216.36s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.066141 -> iter 403: 0.035005,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.20:  56%|█████▌    | 20/36 [1:12:53<57:39, 216.24s/it]  \u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.070159 -> iter 868: 0.041883,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.21:  58%|█████▊    | 21/36 [1:16:30<54:04, 216.28s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 5.183454 -> iter 909: 0.050586,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.22:  61%|██████    | 22/36 [1:20:06<50:26, 216.21s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.118603 -> iter 231: 0.065841,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.23:  64%|██████▍   | 23/36 [1:23:41<46:49, 216.09s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.593550 -> iter 42: 0.095237,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.24:  67%|██████▋   | 24/36 [1:27:17<43:11, 215.99s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.219325 -> iter 895: 0.137932,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.25:  69%|██████▉   | 25/36 [1:30:53<39:35, 215.91s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.305386 -> iter 803: 0.156850,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.26:  72%|███████▏  | 26/36 [1:34:29<35:58, 215.85s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.318905 -> iter 543: 0.209958,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.27:  75%|███████▌  | 27/36 [1:38:04<32:22, 215.84s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.036344 -> iter 582: 0.267911,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.28:  78%|███████▊  | 28/36 [1:41:40<28:46, 215.79s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.614379 -> iter 262: 0.351735,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.29:  81%|████████  | 29/36 [1:45:16<25:10, 215.78s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.727875 -> iter 409: 0.469773,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.30:  83%|████████▎ | 30/36 [1:48:52<21:35, 215.93s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.961505 -> iter 239: 0.581576,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.31:  86%|████████▌ | 31/36 [1:52:29<18:00, 216.17s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.241487 -> iter 686: 0.761975,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.32:  89%|████████▉ | 32/36 [1:56:06<14:25, 216.40s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 3.095121 -> iter 790: 1.080561,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.33:  92%|█████████▏| 33/36 [1:59:43<10:49, 216.65s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 3.884405 -> iter 627: 1.283144,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.34:  94%|█████████▍| 34/36 [2:03:23<07:15, 217.80s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 3.518586 -> iter 562: 1.730482,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing model.language_model.layers.35:  97%|█████████▋| 35/36 [2:07:02<03:37, 217.93s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 8.875799 -> iter 750: 2.585015,'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "Quantizing done: 100%|██████████| 36/36 [2:10:40<00:00, 217.79s/it]                          \n",
      "\u001b[38;20m2026-01-30 19:13:47 INFO device.py L1430: 'peak_ram': 33.24GB, 'peak_vram': 28.59GB\u001b[0m\n",
      "\u001b[38;20m2026-01-30 19:13:47 INFO base.py L1794: quantization tuning time 7902.0025935173035\u001b[0m\n",
      "\u001b[38;20m2026-01-30 19:13:47 INFO base.py L1812: Summary: quantized 252/369 in the model,  ['model.visual.blocks.0.attn.qkv', 'model.visual.blocks.0.attn.proj', 'model.visual.blocks.0.mlp.linear_fc1', 'model.visual.blocks.0.mlp.linear_fc2', 'model.visual.blocks.1.attn.qkv', 'model.visual.blocks.1.attn.proj', 'model.visual.blocks.1.mlp.linear_fc1', 'model.visual.blocks.1.mlp.linear_fc2', 'model.visual.blocks.2.attn.qkv', 'model.visual.blocks.2.attn.proj', 'model.visual.blocks.2.mlp.linear_fc1', 'model.visual.blocks.2.mlp.linear_fc2', 'model.visual.blocks.3.attn.qkv', 'model.visual.blocks.3.attn.proj', 'model.visual.blocks.3.mlp.linear_fc1', 'model.visual.blocks.3.mlp.linear_fc2', 'model.visual.blocks.4.attn.qkv', 'model.visual.blocks.4.attn.proj', 'model.visual.blocks.4.mlp.linear_fc1', 'model.visual.blocks.4.mlp.linear_fc2', 'model.visual.blocks.5.attn.qkv', 'model.visual.blocks.5.attn.proj', 'model.visual.blocks.5.mlp.linear_fc1', 'model.visual.blocks.5.mlp.linear_fc2', 'model.visual.blocks.6.attn.qkv', 'model.visual.blocks.6.attn.proj', 'model.visual.blocks.6.mlp.linear_fc1', 'model.visual.blocks.6.mlp.linear_fc2', 'model.visual.blocks.7.attn.qkv', 'model.visual.blocks.7.attn.proj', 'model.visual.blocks.7.mlp.linear_fc1', 'model.visual.blocks.7.mlp.linear_fc2', 'model.visual.blocks.8.attn.qkv', 'model.visual.blocks.8.attn.proj', 'model.visual.blocks.8.mlp.linear_fc1', 'model.visual.blocks.8.mlp.linear_fc2', 'model.visual.blocks.9.attn.qkv', 'model.visual.blocks.9.attn.proj', 'model.visual.blocks.9.mlp.linear_fc1', 'model.visual.blocks.9.mlp.linear_fc2', 'model.visual.blocks.10.attn.qkv', 'model.visual.blocks.10.attn.proj', 'model.visual.blocks.10.mlp.linear_fc1', 'model.visual.blocks.10.mlp.linear_fc2', 'model.visual.blocks.11.attn.qkv', 'model.visual.blocks.11.attn.proj', 'model.visual.blocks.11.mlp.linear_fc1', 'model.visual.blocks.11.mlp.linear_fc2', 'model.visual.blocks.12.attn.qkv', 'model.visual.blocks.12.attn.proj', 'model.visual.blocks.12.mlp.linear_fc1', 'model.visual.blocks.12.mlp.linear_fc2', 'model.visual.blocks.13.attn.qkv', 'model.visual.blocks.13.attn.proj', 'model.visual.blocks.13.mlp.linear_fc1', 'model.visual.blocks.13.mlp.linear_fc2', 'model.visual.blocks.14.attn.qkv', 'model.visual.blocks.14.attn.proj', 'model.visual.blocks.14.mlp.linear_fc1', 'model.visual.blocks.14.mlp.linear_fc2', 'model.visual.blocks.15.attn.qkv', 'model.visual.blocks.15.attn.proj', 'model.visual.blocks.15.mlp.linear_fc1', 'model.visual.blocks.15.mlp.linear_fc2', 'model.visual.blocks.16.attn.qkv', 'model.visual.blocks.16.attn.proj', 'model.visual.blocks.16.mlp.linear_fc1', 'model.visual.blocks.16.mlp.linear_fc2', 'model.visual.blocks.17.attn.qkv', 'model.visual.blocks.17.attn.proj', 'model.visual.blocks.17.mlp.linear_fc1', 'model.visual.blocks.17.mlp.linear_fc2', 'model.visual.blocks.18.attn.qkv', 'model.visual.blocks.18.attn.proj', 'model.visual.blocks.18.mlp.linear_fc1', 'model.visual.blocks.18.mlp.linear_fc2', 'model.visual.blocks.19.attn.qkv', 'model.visual.blocks.19.attn.proj', 'model.visual.blocks.19.mlp.linear_fc1', 'model.visual.blocks.19.mlp.linear_fc2', 'model.visual.blocks.20.attn.qkv', 'model.visual.blocks.20.attn.proj', 'model.visual.blocks.20.mlp.linear_fc1', 'model.visual.blocks.20.mlp.linear_fc2', 'model.visual.blocks.21.attn.qkv', 'model.visual.blocks.21.attn.proj', 'model.visual.blocks.21.mlp.linear_fc1', 'model.visual.blocks.21.mlp.linear_fc2', 'model.visual.blocks.22.attn.qkv', 'model.visual.blocks.22.attn.proj', 'model.visual.blocks.22.mlp.linear_fc1', 'model.visual.blocks.22.mlp.linear_fc2', 'model.visual.blocks.23.attn.qkv', 'model.visual.blocks.23.attn.proj', 'model.visual.blocks.23.mlp.linear_fc1', 'model.visual.blocks.23.mlp.linear_fc2', 'model.visual.blocks.24.attn.qkv', 'model.visual.blocks.24.attn.proj', 'model.visual.blocks.24.mlp.linear_fc1', 'model.visual.blocks.24.mlp.linear_fc2', 'model.visual.blocks.25.attn.qkv', 'model.visual.blocks.25.attn.proj', 'model.visual.blocks.25.mlp.linear_fc1', 'model.visual.blocks.25.mlp.linear_fc2', 'model.visual.blocks.26.attn.qkv', 'model.visual.blocks.26.attn.proj', 'model.visual.blocks.26.mlp.linear_fc1', 'model.visual.blocks.26.mlp.linear_fc2', 'model.visual.merger.linear_fc1', 'model.visual.merger.linear_fc2', 'model.visual.deepstack_merger_list.0.linear_fc1', 'model.visual.deepstack_merger_list.0.linear_fc2', 'model.visual.deepstack_merger_list.1.linear_fc1', 'model.visual.deepstack_merger_list.1.linear_fc2', 'model.visual.deepstack_merger_list.2.linear_fc1', 'model.visual.deepstack_merger_list.2.linear_fc2', 'lm_head'] have not been quantized\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Qwen3VLForConditionalGeneration(\n",
       "   (model): Qwen3VLModel(\n",
       "     (visual): Qwen3VLVisionModel(\n",
       "       (patch_embed): Qwen3VLVisionPatchEmbed(\n",
       "         (proj): Conv3d(3, 1152, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "       )\n",
       "       (pos_embed): Embedding(2304, 1152)\n",
       "       (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()\n",
       "       (blocks): ModuleList(\n",
       "         (0-26): 27 x Qwen3VLVisionBlock(\n",
       "           (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "           (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "           (attn): Qwen3VLVisionAttention(\n",
       "             (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "             (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "           )\n",
       "           (mlp): Qwen3VLVisionMLP(\n",
       "             (linear_fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "             (linear_fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "             (act_fn): GELUTanh()\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (merger): Qwen3VLVisionPatchMerger(\n",
       "         (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "         (linear_fc1): Linear(in_features=4608, out_features=4608, bias=True)\n",
       "         (act_fn): GELU(approximate='none')\n",
       "         (linear_fc2): Linear(in_features=4608, out_features=4096, bias=True)\n",
       "       )\n",
       "       (deepstack_merger_list): ModuleList(\n",
       "         (0-2): 3 x Qwen3VLVisionPatchMerger(\n",
       "           (norm): LayerNorm((4608,), eps=1e-06, elementwise_affine=True)\n",
       "           (linear_fc1): Linear(in_features=4608, out_features=4608, bias=True)\n",
       "           (act_fn): GELU(approximate='none')\n",
       "           (linear_fc2): Linear(in_features=4608, out_features=4096, bias=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (language_model): Qwen3VLTextModel(\n",
       "       (embed_tokens): Embedding(151936, 4096)\n",
       "       (layers): ModuleList(\n",
       "         (0-35): 36 x Qwen3VLTextDecoderLayer(\n",
       "           (self_attn): Qwen3VLTextAttention(\n",
       "             (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "             (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "             (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "             (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "             (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
       "             (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
       "           )\n",
       "           (mlp): Qwen3VLTextMLP(\n",
       "             (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "             (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "             (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "             (act_fn): SiLUActivation()\n",
       "           )\n",
       "           (input_layernorm): Qwen3VLTextRMSNorm((4096,), eps=1e-06)\n",
       "           (post_attention_layernorm): Qwen3VLTextRMSNorm((4096,), eps=1e-06)\n",
       "         )\n",
       "       )\n",
       "       (norm): Qwen3VLTextRMSNorm((4096,), eps=1e-06)\n",
       "       (rotary_emb): Qwen3VLTextRotaryEmbedding()\n",
       "     )\n",
       "   )\n",
       "   (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       " ),\n",
       " {'model.visual.blocks.0.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.0.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.1.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.1.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.2.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.2.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.3.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.3.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.4.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.4.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.5.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.5.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.6.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.6.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.7.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.7.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.8.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.8.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.9.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.9.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.10.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.10.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.11.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.11.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.12.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.12.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.13.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.13.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.14.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.14.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.15.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.15.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.16.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.16.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.17.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.17.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.18.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.18.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.19.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.19.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.20.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.20.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.21.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.21.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.22.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.22.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.23.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.23.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.24.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.24.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.25.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.25.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.26.mlp.linear_fc1': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.visual.blocks.26.mlp.linear_fc2': {'bits': 16,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'fp',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': True,\n",
       "   'in_blocks': False},\n",
       "  'model.language_model.layers.0.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.0.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.0.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.0.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.0.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.0.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.0.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.1.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.1.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.1.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.1.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.1.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.1.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.1.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.2.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.2.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.2.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.2.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.2.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.2.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.2.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.3.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.3.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.3.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.3.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.3.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.3.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.3.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.4.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.4.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.4.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.4.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.4.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.4.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.4.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.5.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.5.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.5.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.5.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.5.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.5.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.5.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.6.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.6.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.6.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.6.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.6.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.6.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.6.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.7.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.7.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.7.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.7.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.7.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.7.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.7.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.8.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.8.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.8.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.8.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.8.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.8.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.8.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.9.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.9.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.9.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.9.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.9.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.9.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.9.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.10.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.10.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.10.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.10.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.10.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.10.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.10.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.11.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.11.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.11.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.11.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.11.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.11.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.11.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.12.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.12.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.12.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.12.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.12.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.12.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.12.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.13.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.13.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.13.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.13.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.13.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.13.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.13.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.14.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.14.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.14.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.14.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.14.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.14.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.14.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.15.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.15.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.15.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.15.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.15.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.15.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.15.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.16.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.16.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.16.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.16.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.16.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.16.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.16.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.17.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.17.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.17.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.17.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.17.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.17.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.17.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.18.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.18.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.18.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.18.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.18.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.18.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.18.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.19.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.19.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.19.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.19.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.19.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.19.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.19.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.20.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.20.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.20.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.20.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.20.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.20.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.20.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.21.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.21.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.21.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.21.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.21.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.21.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.21.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.22.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.22.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.22.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.22.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.22.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.22.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.22.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.23.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.23.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.23.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.23.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.23.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.23.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.23.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.24.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.24.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.24.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.24.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.24.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.24.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.24.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.25.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.25.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.25.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.25.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.25.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.25.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.25.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.26.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.26.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.26.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.26.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.26.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.26.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.26.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.27.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.27.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.27.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.27.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.27.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.27.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.27.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.28.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.28.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.28.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.28.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.28.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.28.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.28.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.29.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.29.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.29.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.29.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.29.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.29.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.29.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.30.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.30.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.30.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.30.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.30.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.30.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.30.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.31.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.31.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.31.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.31.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.31.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.31.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.31.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.32.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.32.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.32.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.32.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.32.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.32.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.32.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.33.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.33.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.33.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.33.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.33.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.33.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.33.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.34.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.34.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.34.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.34.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.34.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.34.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.34.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.35.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.35.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.35.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.35.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.35.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.35.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.language_model.layers.35.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Exporting to GPTQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "packing model.language_model.layers.35.mlp.down_proj: 100%|██████████| 306/306 [00:07<00:00, 41.30it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hub] Pushing ./AutoRound/W4A16-GPTQ to Vishva007/Qwen3-VL-8B-Instruct-W4A16-AutoRound-GPTQ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cc276132b64be08399d68002ed0450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f92eeb60595414490eb4ba7d8ce9e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hub] ✅ Successfully uploaded: https://huggingface.co/Vishva007/Qwen3-VL-8B-Instruct-W4A16-AutoRound-GPTQ\n"
     ]
    }
   ],
   "source": [
    "base_name = MODEL_ID.split(\"/\")[-1]\n",
    "    \n",
    "if \"AWQ\" in TARGET_FORMATS:\n",
    "        save_dir = os.path.join(OUTPUT_BASE_DIR, \"W4A16-AWQ\")\n",
    "        print(\"\\n💾 Exporting to AWQ...\")\n",
    "        ar.save_quantized(save_dir, format=\"auto_awq\", inplace=False)\n",
    "        push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound-AWQ\", HF_TOKEN)\n",
    "\n",
    "if \"GPTQ\" in TARGET_FORMATS:\n",
    "        save_dir = os.path.join(OUTPUT_BASE_DIR, \"W4A16-GPTQ\")\n",
    "        print(\"\\n💾 Exporting to GPTQ...\")\n",
    "        ar.save_quantized(save_dir, format=\"auto_gptq\", inplace=False)\n",
    "        push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound-GPTQ\", HF_TOKEN)\n",
    "\n",
    "if \"AutoRound\" in TARGET_FORMATS:\n",
    "        save_dir = os.path.join(OUTPUT_BASE_DIR, \"W4A16-AutoRound\")\n",
    "        print(\"\\n💾 Exporting to AutoRound format...\")\n",
    "        ar.save_quantized(save_dir, format=\"auto_round\", inplace=False)\n",
    "        push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound\", HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc4478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
