{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8040c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from auto_round import AutoRound\n",
    "from huggingface_hub import HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cddaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6517109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n",
      "VRAM: 23.5 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b05d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "HF_USER = \"Vishva007\"\n",
    "OUTPUT_BASE_DIR = \"./AutoRound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4920035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1063ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FORMATS = [\n",
    "    \"AWQ\",            # Best for Nvidia GPUs (vLLM, TGI)\n",
    "    # \"GPTQ\",         # Good compatibility\n",
    "    \"AutoRound\",      # Intel default format (Requires auto-round lib to run)\n",
    "    # \"GGUF\",         # For llama.cpp / Olama\n",
    "]\n",
    "\n",
    "# High-End GPU Tuning Parameters (A40/A6000/L40)\n",
    "TUNING_CONFIG = {\n",
    "    \"group_size\": 128,\n",
    "    \"sym\": True,\n",
    "    \"iters\": 1000,          # High accuracy (Production grade)\n",
    "    \"nsamples\": 512,        # More calibration data\n",
    "    \"batch_size\": 8,        # Faster on 48GB VRAM\n",
    "    \"seqlen\": 2048,\n",
    "    \"low_gpu_mem_usage\": False,   # Keep on GPU for speed\n",
    "    \"enable_torch_compile\": True, # JIT acceleration\n",
    "    \"quant_nontext_module\": False # Keep Vision Tower in BF16 (Crucial for VLM accuracy)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(local_dir, repo_name, token):\n",
    "    \"\"\"Creates repo and uploads folder to Hugging Face.\"\"\"\n",
    "    full_repo_id = f\"{HF_USER}/{repo_name}\"\n",
    "    print(f\"\\n[Hub] Pushing {local_dir} to {full_repo_id}...\")\n",
    "    \n",
    "    try:\n",
    "        api = HfApi(token=token)\n",
    "        create_repo(full_repo_id, repo_type=\"model\", exist_ok=True, private=False, token=token)\n",
    "        \n",
    "        api.upload_folder(\n",
    "            folder_path=local_dir,\n",
    "            repo_id=full_repo_id,\n",
    "            repo_type=\"model\",\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"[Hub] ✅ Successfully uploaded: https://huggingface.co/{full_repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Hub] ❌ Error uploading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00d0a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eaae545f204af38f4fe6aa23935e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c27541a16c4ef89dc090d9b0c8553c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 Client Error. (Request ID: Root=1-697cd268-03c062925dfe3dd45fefea95;11f64133-6eb0-40b8-b08e-c6d2bd09520a)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/model_index.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5d7796e97b46738ac98f54715845f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m2026-01-30 15:46:51 WARNING base.py L291: unrecognized keys ['quant_nontext_module'] were passed. Please check them.\u001b[0m\n",
      "\u001b[38;20m2026-01-30 15:46:51 INFO base.py L391: using torch.bfloat16 for quantization tuning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ar = AutoRound(\n",
    "        MODEL_ID,\n",
    "        scheme=\"W4A16\",\n",
    "        **TUNING_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m2026-01-30 15:47:35 WARNING base.py L1685: this API is deprecated, please use `quantize_and_save` instead\u001b[0m\n",
      "\u001b[38;20m2026-01-30 15:47:35 INFO base.py L1729: start to cache block inputs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d9d66c91b444ccada1262b24f5a549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dbe0d20861411b9ee242d2bd470fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74d538c5b57471aaec53b2041e59a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-4746b8785c874c(…):   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d1daa66136462a84fdf05836167627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907bb6114b134d2bbbc3dc1ead9e21d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7d9d62b77f4ec8a1c26821f8928ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bad2b23d9c44d3b9f7752a02f9d20dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2026-01-30 15:48:01 INFO base.py L1744: caching done\u001b[0m\n",
      "Quantizing model.layers.0:   0%|          | 0/36 [00:01<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:829: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:110.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000156 -> iter 738: 0.000056,'peak_ram': 14.75GB, 'peak_vram': 19.75GB\u001b[0m\n",
      "Quantizing model.layers.1:   3%|▎         | 1/36 [01:59<1:09:35, 119.30s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000382 -> iter 728: 0.000122,'peak_ram': 14.75GB, 'peak_vram': 20.81GB\u001b[0m\n",
      "Quantizing model.layers.2:   6%|▌         | 2/36 [03:25<56:38, 99.96s/it]   \u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000525 -> iter 497: 0.000148,'peak_ram': 14.75GB, 'peak_vram': 20.83GB\u001b[0m\n",
      "Quantizing model.layers.3:   8%|▊         | 3/36 [04:51<51:31, 93.67s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.000944 -> iter 845: 0.000331,'peak_ram': 14.75GB, 'peak_vram': 20.83GB\u001b[0m\n",
      "Quantizing model.layers.4:  11%|█         | 4/36 [06:18<48:22, 90.72s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.001574 -> iter 784: 0.000601,'peak_ram': 14.75GB, 'peak_vram': 20.83GB\u001b[0m\n",
      "Quantizing model.layers.5:  14%|█▍        | 5/36 [07:44<45:59, 89.03s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.002474 -> iter 539: 0.001012,'peak_ram': 14.75GB, 'peak_vram': 20.83GB\u001b[0m\n",
      "Quantizing model.layers.6:  17%|█▋        | 6/36 [09:10<43:59, 87.99s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.023304 -> iter 967: 0.002494,'peak_ram': 14.75GB, 'peak_vram': 20.83GB\u001b[0m\n",
      "Quantizing model.layers.7:  19%|█▉        | 7/36 [10:36<42:14, 87.38s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.007215 -> iter 731: 0.003252,'peak_ram': 14.75GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.8:  22%|██▏       | 8/36 [12:02<40:35, 87.00s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.006858 -> iter 839: 0.004013,'peak_ram': 14.75GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.9:  25%|██▌       | 9/36 [13:28<39:02, 86.74s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.009022 -> iter 891: 0.004429,'peak_ram': 14.75GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.10:  28%|██▊       | 10/36 [14:54<37:29, 86.53s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.007650 -> iter 648: 0.004321,'peak_ram': 14.75GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.11:  31%|███       | 11/36 [16:20<35:59, 86.38s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.007381 -> iter 661: 0.004713,'peak_ram': 14.75GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.12:  33%|███▎      | 12/36 [17:46<34:29, 86.24s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.008632 -> iter 962: 0.005161,'peak_ram': 15.07GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.13:  36%|███▌      | 13/36 [19:12<33:02, 86.18s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.009573 -> iter 854: 0.005892,'peak_ram': 15.45GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.14:  39%|███▉      | 14/36 [20:38<31:34, 86.13s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.009886 -> iter 941: 0.006056,'peak_ram': 15.82GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.15:  42%|████▏     | 15/36 [22:04<30:08, 86.10s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.009906 -> iter 675: 0.007336,'peak_ram': 16.21GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.16:  44%|████▍     | 16/36 [23:30<28:41, 86.07s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.369209 -> iter 867: 0.019423,'peak_ram': 16.57GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.17:  47%|████▋     | 17/36 [24:56<27:14, 86.05s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.322779 -> iter 996: 0.018518,'peak_ram': 16.97GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.18:  50%|█████     | 18/36 [26:22<25:48, 86.01s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.040333 -> iter 353: 0.019546,'peak_ram': 17.33GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.19:  53%|█████▎    | 19/36 [27:48<24:22, 86.01s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.046849 -> iter 570: 0.025614,'peak_ram': 17.69GB, 'peak_vram': 20.88GB\u001b[0m\n",
      "Quantizing model.layers.20:  56%|█████▌    | 20/36 [29:14<22:56, 86.00s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.045287 -> iter 431: 0.029522,'peak_ram': 18.08GB, 'peak_vram': 21.05GB\u001b[0m\n",
      "Quantizing model.layers.21:  58%|█████▊    | 21/36 [30:40<21:29, 85.99s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.876964 -> iter 886: 0.034700,'peak_ram': 18.47GB, 'peak_vram': 21.05GB\u001b[0m\n",
      "Quantizing model.layers.22:  61%|██████    | 22/36 [32:06<20:03, 86.00s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.072215 -> iter 834: 0.041044,'peak_ram': 18.84GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.23:  64%|██████▍   | 23/36 [33:32<18:37, 85.97s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.160794 -> iter 580: 0.051544,'peak_ram': 19.2GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.24:  67%|██████▋   | 24/36 [34:58<17:11, 85.95s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.057674 -> iter 457: 0.071097,'peak_ram': 19.59GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.25:  69%|██████▉   | 25/36 [36:24<15:45, 85.94s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.948129 -> iter 803: 0.079919,'peak_ram': 19.97GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.26:  72%|███████▏  | 26/36 [37:50<14:19, 85.96s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.019297 -> iter 273: 0.104348,'peak_ram': 20.33GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.27:  75%|███████▌  | 27/36 [39:16<12:53, 86.00s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.180356 -> iter 170: 0.124785,'peak_ram': 20.72GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.28:  78%|███████▊  | 28/36 [40:42<11:28, 86.02s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.294956 -> iter 396: 0.181976,'peak_ram': 21.09GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.29:  81%|████████  | 29/36 [42:08<10:02, 86.02s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.317386 -> iter 864: 0.233593,'peak_ram': 21.47GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.30:  83%|████████▎ | 30/36 [43:34<08:36, 86.02s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 0.348188 -> iter 198: 0.289065,'peak_ram': 21.85GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.31:  86%|████████▌ | 31/36 [45:00<07:10, 86.04s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.732784 -> iter 88: 0.357463,'peak_ram': 22.23GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.32:  89%|████████▉ | 32/36 [46:26<05:44, 86.04s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 2.705243 -> iter 256: 0.520515,'peak_ram': 22.61GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.33:  92%|█████████▏| 33/36 [47:52<04:18, 86.06s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.494152 -> iter 627: 0.668937,'peak_ram': 22.99GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.34:  94%|█████████▍| 34/36 [49:18<02:52, 86.07s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 1.859856 -> iter 562: 0.878458,'peak_ram': 23.37GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing model.layers.35:  97%|█████████▋| 35/36 [50:45<01:26, 86.14s/it]\u001b[38;20mquantized 7/7 layers in the block, loss iter 0: 4.211696 -> iter 103: 1.877001,'peak_ram': 23.37GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "Quantizing done: 100%|██████████| 36/36 [52:11<00:00, 87.00s/it]           \n",
      "\u001b[38;20m2026-01-30 16:40:13 INFO device.py L1430: 'peak_ram': 23.37GB, 'peak_vram': 21.17GB\u001b[0m\n",
      "\u001b[38;20m2026-01-30 16:40:13 INFO base.py L1794: quantization tuning time 3157.569826364517\u001b[0m\n",
      "\u001b[38;20m2026-01-30 16:40:13 INFO base.py L1812: Summary: quantized 252/253 in the model,  ['lm_head'] have not been quantized\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Qwen3ForCausalLM(\n",
       "   (model): Qwen3Model(\n",
       "     (embed_tokens): Embedding(151936, 2560)\n",
       "     (layers): ModuleList(\n",
       "       (0-35): 36 x Qwen3DecoderLayer(\n",
       "         (self_attn): Qwen3Attention(\n",
       "           (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "           (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "           (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "           (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "           (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "           (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "         )\n",
       "         (mlp): Qwen3MLP(\n",
       "           (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "           (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "           (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "           (act_fn): SiLUActivation()\n",
       "         )\n",
       "         (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "         (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "     (rotary_emb): Qwen3RotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       " ),\n",
       " {'model.layers.0.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.0.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.0.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.0.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.0.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.0.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.0.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.1.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.1.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.1.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.1.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.1.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.1.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.1.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.2.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.2.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.2.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.2.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.2.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.2.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.2.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.3.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.3.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.3.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.3.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.3.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.3.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.3.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.4.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.4.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.4.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.4.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.4.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.4.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.4.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.5.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.5.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.5.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.5.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.5.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.5.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.5.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.6.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.6.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.6.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.6.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.6.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.6.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.6.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.7.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.7.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.7.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.7.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.7.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.7.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.7.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.8.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.8.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.8.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.8.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.8.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.8.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.8.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.9.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.9.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.9.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.9.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.9.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.9.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.9.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.10.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.10.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.10.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.10.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.10.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.10.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.10.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.11.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.11.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.11.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.11.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.11.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.11.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.11.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.12.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.12.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.12.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.12.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.12.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.12.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.12.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.13.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.13.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.13.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.13.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.13.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.13.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.13.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.14.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.14.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.14.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.14.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.14.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.14.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.14.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.15.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.15.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.15.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.15.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.15.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.15.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.15.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.16.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.16.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.16.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.16.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.16.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.16.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.16.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.17.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.17.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.17.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.17.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.17.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.17.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.17.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.18.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.18.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.18.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.18.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.18.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.18.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.18.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.19.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.19.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.19.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.19.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.19.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.19.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.19.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.20.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.20.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.20.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.20.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.20.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.20.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.20.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.21.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.21.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.21.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.21.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.21.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.21.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.21.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.22.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.22.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.22.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.22.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.22.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.22.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.22.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.23.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.23.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.23.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.23.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.23.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.23.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.23.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.24.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.24.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.24.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.24.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.24.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.24.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.24.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.25.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.25.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.25.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.25.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.25.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.25.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.25.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.26.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.26.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.26.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.26.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.26.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.26.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.26.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.27.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.27.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.27.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.27.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.27.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.27.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.27.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.28.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.28.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.28.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.28.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.28.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.28.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.28.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.29.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.29.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.29.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.29.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.29.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.29.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.29.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.30.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.30.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.30.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.30.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.30.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.30.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.30.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.31.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.31.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.31.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.31.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.31.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.31.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.31.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.32.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.32.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.32.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.32.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.32.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.32.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.32.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.33.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.33.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.33.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.33.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.33.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.33.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.33.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.34.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.34.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.34.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.34.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.34.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.34.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.34.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.35.self_attn.q_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.35.self_attn.k_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.35.self_attn.v_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.35.self_attn.o_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.35.mlp.gate_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.35.mlp.up_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True},\n",
       "  'model.layers.35.mlp.down_proj': {'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'data_type': 'int',\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': None,\n",
       "   'act_sym': None,\n",
       "   'act_data_type': None,\n",
       "   'act_dynamic': None,\n",
       "   'super_bits': None,\n",
       "   'super_group_size': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'fixed_by_user': False,\n",
       "   'in_blocks': True}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m2026-01-30 16:43:09 WARNING export.py L94: ./AutoRound/W4A16-AWQ already exists, this may cause model conflict\u001b[0m\n",
      "\u001b[38;20m2026-01-30 16:43:09 INFO export.py L96: Saving quantized model to auto_awq format\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Exporting to AWQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "packing model.layers.35.mlp.down_proj: 100%|██████████| 252/252 [00:03<00:00, 74.00it/s]   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9615566b6b414088f78d7dfbe7526a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m2026-01-30 16:43:19 WARNING utils.py L79: Skipping source model Python file copy due to error: cannot import name 'TRANSFORMERS_CACHE' from 'transformers' (/opt/conda/lib/python3.11/site-packages/transformers/__init__.py)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hub] Pushing ./AutoRound/W4A16-AWQ to Vishva007/Qwen3-4B-Instruct-2507-W4A16-AutoRound-AWQ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fd91efd60e42bca68c4ede11a8c645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686cd5252a4d45b78ba3077f97ea5e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hub] ✅ Successfully uploaded: https://huggingface.co/Vishva007/Qwen3-4B-Instruct-2507-W4A16-AutoRound-AWQ\n",
      "\n",
      "💾 Exporting to AutoRound format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "packing model.layers.35.mlp.down_proj: 100%|██████████| 252/252 [00:04<00:00, 62.01it/s]   \n",
      "\u001b[33;1m2026-01-30 16:43:53 WARNING export.py L379: ./AutoRound/W4A16-AutoRound already exists, this may cause model conflict\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba76957b4d5d4aba80b601883d634ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m2026-01-30 16:43:54 WARNING utils.py L79: Skipping source model Python file copy due to error: cannot import name 'TRANSFORMERS_CACHE' from 'transformers' (/opt/conda/lib/python3.11/site-packages/transformers/__init__.py)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hub] Pushing ./AutoRound/W4A16-AutoRound to Vishva007/Qwen3-4B-Instruct-2507-W4A16-AutoRound...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9830e90c2da24fc69a16777b9b1d9867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9920ef8e49404c66a4fd806a262a9733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hub] ✅ Successfully uploaded: https://huggingface.co/Vishva007/Qwen3-4B-Instruct-2507-W4A16-AutoRound\n"
     ]
    }
   ],
   "source": [
    "base_name = MODEL_ID.split(\"/\")[-1]\n",
    "    \n",
    "if \"AWQ\" in TARGET_FORMATS:\n",
    "        save_dir = os.path.join(OUTPUT_BASE_DIR, \"W4A16-AWQ\")\n",
    "        print(\"\\n💾 Exporting to AWQ...\")\n",
    "        ar.save_quantized(save_dir, format=\"auto_awq\", inplace=False)\n",
    "        push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound-AWQ\", HF_TOKEN)\n",
    "\n",
    "if \"GPTQ\" in TARGET_FORMATS:\n",
    "        save_dir = os.path.join(OUTPUT_BASE_DIR, \"W4A16-GPTQ\")\n",
    "        print(\"\\n💾 Exporting to GPTQ...\")\n",
    "        ar.save_quantized(save_dir, format=\"auto_gptq\", inplace=False)\n",
    "        push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound-GPTQ\", HF_TOKEN)\n",
    "\n",
    "if \"AutoRound\" in TARGET_FORMATS:\n",
    "        save_dir = os.path.join(OUTPUT_BASE_DIR, \"W4A16-AutoRound\")\n",
    "        print(\"\\n💾 Exporting to AutoRound format...\")\n",
    "        ar.save_quantized(save_dir, format=\"auto_round\", inplace=False)\n",
    "        push_to_hub(save_dir, f\"{base_name}-W4A16-AutoRound\", HF_TOKEN)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
